{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_0',\n",
       " 'train_1',\n",
       " 'train_2',\n",
       " 'train_3',\n",
       " 'train_4',\n",
       " 'train_5',\n",
       " 'train_6',\n",
       " 'train_7',\n",
       " 'train_8',\n",
       " 'train_9']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['id'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['dialogue'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a check-up\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['topic'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b18642a0c446acb5ccac8288a912d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d94e4feb7348d4a54fe339ae58086a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad97fbb4bfac49bba5add0ea2f832ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcc84f93bbc44d080b623f865e09d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc384beb32c4efb9316720e61023430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c2d81ecea345b2a244af2ccd8c5324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76745df0f50c4a0896ec90ccac992fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([[ 363,   97,   19,   34,    6, 3059,   58,    1]])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "    sentence_encoded['input_ids'][0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded['input_ids'])\n",
    "print(\"\\nDECODED SENTENCE:\")\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids  :  tensor([[ 363,   97,   19,   34,    6, 3059,   58,    1]])\n",
      "attention_mask  :  tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for k, v in sentence_encoded.items():\n",
    "    print(k, \" : \",  v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7:00 PM'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(\n",
    "        sentence_encoded['input_ids'], \n",
    "        max_new_tokens=50\n",
    "    )[0].tolist(), \n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Exmaple 1\n",
      "INPUT PROMPT:\n",
      "#Person1#: I just bought a new dress. What do you think of it?\n",
      "#Person2#: You look really great in it. So are you going to a job interview or a party?\n",
      "#Person1#: No, I was invited to give a talk in my school.\n",
      "#Person2#: So how much did you pay for it?\n",
      "#Person1#: I pay just $70 for it. I saved $30.\n",
      "#Person2#: That's really a bargain.\n",
      "#Person1#: You're right. Well, what did you do while I was out shopping?\n",
      "#Person2#: I watched TV for a while and then I did some reading. It wasn't a very interesting book so I just read a few pages. Then I took a shower.\n",
      "#Person1#: I thought you said you were going to see Mike.\n",
      "#Person2#: I'll go and visit him at his home tomorrow. He'll return home tomorrow morning.\n",
      "#Person1#: I'm glad he can finally returned home after that accident.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "While #Person1# made a bargain to buy a new dress, #Person2# watched TV, read a boring book, and took a shower at home.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION:\n",
      "#Person2#: I'm glad he returned home.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Exmaple 2\n",
      "INPUT PROMPT:\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION:\n",
      "I am a student at the university.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['train'][index]['dialogue']\n",
    "    summary = dataset['train'][index]['summary']\n",
    "\n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_new_tokens = 50\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Exmaple', i + 1)\n",
    "    print(f\"INPUT PROMPT:\\n{dialogue}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Exmaple 1\n",
      "INPUT PROMPT:\n",
      "#Person1#: I just bought a new dress. What do you think of it?\n",
      "#Person2#: You look really great in it. So are you going to a job interview or a party?\n",
      "#Person1#: No, I was invited to give a talk in my school.\n",
      "#Person2#: So how much did you pay for it?\n",
      "#Person1#: I pay just $70 for it. I saved $30.\n",
      "#Person2#: That's really a bargain.\n",
      "#Person1#: You're right. Well, what did you do while I was out shopping?\n",
      "#Person2#: I watched TV for a while and then I did some reading. It wasn't a very interesting book so I just read a few pages. Then I took a shower.\n",
      "#Person1#: I thought you said you were going to see Mike.\n",
      "#Person2#: I'll go and visit him at his home tomorrow. He'll return home tomorrow morning.\n",
      "#Person1#: I'm glad he can finally returned home after that accident.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "While #Person1# made a bargain to buy a new dress, #Person2# watched TV, read a boring book, and took a shower at home.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION:\n",
      "What did you do while you were out shopping?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Exmaple 2\n",
      "INPUT PROMPT:\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION:\n",
      "What is your school's average grade?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['train'][index]['dialogue']\n",
    "    summary = dataset['train'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_new_tokens=50\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Exmaple', i + 1)\n",
    "    print(f\"INPUT PROMPT:\\n{dialogue}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Inference with Prompt Template from FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Exmaple 1\n",
      "INPUT PROMPT:\n",
      "#Person1#: I just bought a new dress. What do you think of it?\n",
      "#Person2#: You look really great in it. So are you going to a job interview or a party?\n",
      "#Person1#: No, I was invited to give a talk in my school.\n",
      "#Person2#: So how much did you pay for it?\n",
      "#Person1#: I pay just $70 for it. I saved $30.\n",
      "#Person2#: That's really a bargain.\n",
      "#Person1#: You're right. Well, what did you do while I was out shopping?\n",
      "#Person2#: I watched TV for a while and then I did some reading. It wasn't a very interesting book so I just read a few pages. Then I took a shower.\n",
      "#Person1#: I thought you said you were going to see Mike.\n",
      "#Person2#: I'll go and visit him at his home tomorrow. He'll return home tomorrow morning.\n",
      "#Person1#: I'm glad he can finally returned home after that accident.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "While #Person1# made a bargain to buy a new dress, #Person2# watched TV, read a boring book, and took a shower at home.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION:\n",
      "Mike is back home.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Exmaple 2\n",
      "INPUT PROMPT:\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION:\n",
      "Person1#: I am a student at the university.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['train'][index]['dialogue']\n",
    "    summary = dataset['train'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Exmaple', i + 1)\n",
    "    print(f\"INPUT PROMPT:\\n{dialogue}\")\n",
    "    print(dash_line)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dash_line)\n",
    "    print(f\"MODEL GENERATION:\\n{output}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['train'][index]['dialogue']\n",
    "        summary = dataset['train'][index]['summary']\n",
    "\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    dialogue = dataset['train'][example_index_to_summarize]['dialogue']\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: I just bought a new dress. What do you think of it?\n",
      "#Person2#: You look really great in it. So are you going to a job interview or a party?\n",
      "#Person1#: No, I was invited to give a talk in my school.\n",
      "#Person2#: So how much did you pay for it?\n",
      "#Person1#: I pay just $70 for it. I saved $30.\n",
      "#Person2#: That's really a bargain.\n",
      "#Person1#: You're right. Well, what did you do while I was out shopping?\n",
      "#Person2#: I watched TV for a while and then I did some reading. It wasn't a very interesting book so I just read a few pages. Then I took a shower.\n",
      "#Person1#: I thought you said you were going to see Mike.\n",
      "#Person2#: I'll go and visit him at his home tomorrow. He'll return home tomorrow morning.\n",
      "#Person1#: I'm glad he can finally returned home after that accident.\n",
      "\n",
      "What was going on?\n",
      "While #Person1# made a bargain to buy a new dress, #Person2# watched TV, read a boring book, and took a shower at home.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "Person1#: I am a student at college.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['train'][example_index_to_summarize]['summary']\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=50\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: I just bought a new dress. What do you think of it?\n",
      "#Person2#: You look really great in it. So are you going to a job interview or a party?\n",
      "#Person1#: No, I was invited to give a talk in my school.\n",
      "#Person2#: So how much did you pay for it?\n",
      "#Person1#: I pay just $70 for it. I saved $30.\n",
      "#Person2#: That's really a bargain.\n",
      "#Person1#: You're right. Well, what did you do while I was out shopping?\n",
      "#Person2#: I watched TV for a while and then I did some reading. It wasn't a very interesting book so I just read a few pages. Then I took a shower.\n",
      "#Person1#: I thought you said you were going to see Mike.\n",
      "#Person2#: I'll go and visit him at his home tomorrow. He'll return home tomorrow morning.\n",
      "#Person1#: I'm glad he can finally returned home after that accident.\n",
      "\n",
      "What was going on?\n",
      "While #Person1# made a bargain to buy a new dress, #Person2# watched TV, read a boring book, and took a shower at home.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Hey, man. What's up?\n",
      "#Person2#: Ah, first of all, I put a buck in the vending machine for a seventy-five cent candy bar, and the thing got stuck here in the machine. Then, I pressed the change button [Ah, man], and nothing happened. [Wow!] Nothing came out. The dumb thing still owes me a quarter.\n",
      "#Person1#: Well, did you talk to the man at the snack bar to see if he could refund your money?\n",
      "#Person2#: Yeah, I tried that, but he said he didn't own the machine, and I'd have to call the phone number on the machine.\n",
      "#Person1#: What a bummer.\n",
      "#Person2#: Hey, I have an idea. [What?] Why don't we rock the machine back and forth until the candy bar falls?\n",
      "#Person1#: Nothing doing. I don't want to be responsible for breaking the thing, and besides, someone might call the cops.\n",
      "#Person2#: Ah, don't worry. I've done it before. Oh well. Hey, hey, tough luck. Hey, here, take my candy bar. [You mean?] Yeah, the machine and I hit it off earlier today.\n",
      "\n",
      "What was going on?\n",
      "#Person2# tells #Person1# he put a buck in the vending machine but the thing got stuck. After hitting the machine, #Person2# gets the candy bar.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: I'm sorry, I've lost my tags and receipt. What can I do about it?\n",
      "#Person2#: I see. What is your baggage? And do you remember the tag's number or color?\n",
      "#Person1#: It's a suitcase. Its colour is blue.\n",
      "#Person2#: Could you give me a description of your case?\n",
      "#Person1#: It's like this, it's square with a leather cover.\n",
      "#Person2#: I'll check it for you. I've found it. Is this yours?\n",
      "#Person1#: Yes, it is the right one\n",
      "#Person2#: Will you show me your key card, please?\n",
      "#Person1#: Here it is.\n",
      "#Person2#: All right. Now you can take your case away\n",
      "\n",
      "What was going on?\n",
      "#Person1# lost #Person1#'s tags and receipt. #Person2# checks and helps #Person1# find #Person1#'s case.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "In any booze competition that you donâ€™t work hard at it really works to improve it while at it you are with your school classmates. So the he thinks you ought\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['train'][example_index_to_summarize]['summary']\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=2.0\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Configuration Parameters for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['generative_config'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m generative_config \u001b[38;5;241m=\u001b[39m GenerationConfig(max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(few_shot_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerative_config\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      9\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(dash_line)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBASELINE HUMAN SUMMARY:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msummary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1307\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1307\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1122\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1125\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['generative_config'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "generative_config = GenerationConfig(max_new_tokens=50)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'],\n",
    "        generative_config=generative_config\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
